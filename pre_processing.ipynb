{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_points = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def get_data(csv):\\n    data = pd.read_csv(csv)\\n    data = data.drop('date', axis=1)\\n    data = data.drop(0, axis=0)\\n    data = data.values\\n    \\n    data_normaliser = preprocessing.MinMaxScaler()\\n    data_normalised = data_normaliser.fit_transform(data)\\n    \\n    #predict future patterns using past data_points\\n    points_normalised = np.array([data_normalised[i:i + data_points].copy() for i in range(len(data_normalised) - data_points)])\\n    open_values_normalised = np.array([data_normalised[:, 0][i + data_points].copy() for i in range(len(data_normalised) - data_points)])\\n    open_values_normalised = np.expand_dims(open_values_normalised, -1)\\n\\n    open_values = np.array([data[:, 0][i + data_points].copy() for i in range(len(data) - data_points)])\\n    open_values = np.expand_dims(open_values, -1)\\n\\n    normaliser = preprocessing.MinMaxScaler()\\n    normaliser.fit(open_values)\\n\\n    def calc_ema(values, timeline):\\n        sma = np.mean(values[:, 3])\\n        ema = [sma]\\n        k = 2 / (1 + timeline)\\n        for i in range(len(past_price) - timeline, len(his)):\\n            close = past_price[i][3]\\n            ema.append(close * k + ema[-1] * (1 - k))\\n        return ema[-1]\\n\\n    indicators = []\\n    for past_price in points_normalised:\\n        sma = np.mean(past_price[:, 3])\\n        macd = calculate_ema(past_price, 12) - calculate_ema(past_price, 26)\\n        indicators.append(np.array([sma]))\\n\\n    indicators = np.array(indicators)\\n\\n    scaler = preprocessing.MinMaxScaler()\\n    indicators_normalised = scaler.fit_transform(indicators)\\n\\n    assert points_normalised.shape[0] == open_values_normalised.shape[0] == indicators_normalised.shape[0]\\n    return points_normalised, indicators_normalised, open_values_normalised, open_values, normaliser\\n\\n\\ndef get_large_data(test_set_name):\\n    import os\\n    past_points = 0\\n    indicators = 0\\n    open_values = 0\\n    for csv_file_path in list(filter(lambda x: x.endswith('daily.csv'), os.listdir('./Users\\\\Josh\\\\Documents\\x04th Year Docs\\\\Dist+Con\\\\CA Group\\\\StockcsvsStockcsvs'))):\\n        if not csv_file_path == test_set_name:\\n            print(csv_file_path)\\n            if type(past_points) == int:\\n                past_points, indicators, open_values, _, _ = get_data(csv_file_path)\\n            else:\\n                a, b, c, _, _ = get_data(csv_file_path)\\n                past_points = np.concatenate((past_points, a), 0)\\n                indicators = np.concatenate((indicators, b), 0)\\n                open_values = np.concatenate((open_values, c), 0)\\n\\n    train_past_points = past_points\\n    train_indicator = indicators\\n    training = open_values\\n\\n    past_points_test, indicator_test, test, unscaled_test, normaliser = get_data(test_set_name)\\n\\n    return train_past_points, train_indicator, training, past_points_test, indicator_test, test, unscaled_test, normaliser\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def get_data(csv):\n",
    "    data = pd.read_csv(csv)\n",
    "    data = data.drop('date', axis=1)\n",
    "    data = data.drop(0, axis=0)\n",
    "    data = data.values\n",
    "    \n",
    "    data_normaliser = preprocessing.MinMaxScaler()\n",
    "    data_normalised = data_normaliser.fit_transform(data)\n",
    "    \n",
    "    #predict future patterns using past data_points\n",
    "    points_normalised = np.array([data_normalised[i:i + data_points].copy() for i in range(len(data_normalised) - data_points)])\n",
    "    open_values_normalised = np.array([data_normalised[:, 0][i + data_points].copy() for i in range(len(data_normalised) - data_points)])\n",
    "    open_values_normalised = np.expand_dims(open_values_normalised, -1)\n",
    "\n",
    "    open_values = np.array([data[:, 0][i + data_points].copy() for i in range(len(data) - data_points)])\n",
    "    open_values = np.expand_dims(open_values, -1)\n",
    "\n",
    "    normaliser = preprocessing.MinMaxScaler()\n",
    "    normaliser.fit(open_values)\n",
    "\n",
    "    def calc_ema(values, timeline):\n",
    "        sma = np.mean(values[:, 3])\n",
    "        ema = [sma]\n",
    "        k = 2 / (1 + timeline)\n",
    "        for i in range(len(past_price) - timeline, len(his)):\n",
    "            close = past_price[i][3]\n",
    "            ema.append(close * k + ema[-1] * (1 - k))\n",
    "        return ema[-1]\n",
    "\n",
    "    indicators = []\n",
    "    for past_price in points_normalised:\n",
    "        sma = np.mean(past_price[:, 3])\n",
    "        macd = calculate_ema(past_price, 12) - calculate_ema(past_price, 26)\n",
    "        indicators.append(np.array([sma]))\n",
    "\n",
    "    indicators = np.array(indicators)\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    indicators_normalised = scaler.fit_transform(indicators)\n",
    "\n",
    "    assert points_normalised.shape[0] == open_values_normalised.shape[0] == indicators_normalised.shape[0]\n",
    "    return points_normalised, indicators_normalised, open_values_normalised, open_values, normaliser\n",
    "\n",
    "\n",
    "def get_large_data(test_set_name):\n",
    "    import os\n",
    "    past_points = 0\n",
    "    indicators = 0\n",
    "    open_values = 0\n",
    "    for csv_file_path in list(filter(lambda x: x.endswith('daily.csv'), os.listdir('./Users\\Josh\\Documents\\4th Year Docs\\Dist+Con\\CA Group\\StockcsvsStockcsvs'))):\n",
    "        if not csv_file_path == test_set_name:\n",
    "            print(csv_file_path)\n",
    "            if type(past_points) == int:\n",
    "                past_points, indicators, open_values, _, _ = get_data(csv_file_path)\n",
    "            else:\n",
    "                a, b, c, _, _ = get_data(csv_file_path)\n",
    "                past_points = np.concatenate((past_points, a), 0)\n",
    "                indicators = np.concatenate((indicators, b), 0)\n",
    "                open_values = np.concatenate((open_values, c), 0)\n",
    "\n",
    "    train_past_points = past_points\n",
    "    train_indicator = indicators\n",
    "    training = open_values\n",
    "\n",
    "    past_points_test, indicator_test, test, unscaled_test, normaliser = get_data(test_set_name)\n",
    "\n",
    "    return train_past_points, train_indicator, training, past_points_test, indicator_test, test, unscaled_test, normaliser\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "history_points = 50\n",
    "\n",
    "\n",
    "def csv_to_dataset(csv_path):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    data = data.drop('date', axis=1)\n",
    "    data = data.drop(0, axis=0)\n",
    "\n",
    "    data = data.values\n",
    "\n",
    "    data_normaliser = preprocessing.MinMaxScaler()\n",
    "    data_normalised = data_normaliser.fit_transform(data)\n",
    "\n",
    "    # using the last {history_points} open close high low volume data points, predict the next open value\n",
    "    ohlcv_histories_normalised = np.array([data_normalised[i:i + history_points].copy() for i in range(len(data_normalised) - history_points)])\n",
    "    next_day_open_values_normalised = np.array([data_normalised[:, 0][i + history_points].copy() for i in range(len(data_normalised) - history_points)])\n",
    "    next_day_open_values_normalised = np.expand_dims(next_day_open_values_normalised, -1)\n",
    "\n",
    "    next_day_open_values = np.array([data[:, 0][i + history_points].copy() for i in range(len(data) - history_points)])\n",
    "    next_day_open_values = np.expand_dims(next_day_open_values, -1)\n",
    "\n",
    "    y_normaliser = preprocessing.MinMaxScaler()\n",
    "    y_normaliser.fit(next_day_open_values)\n",
    "\n",
    "    def calc_ema(values, time_period):\n",
    "        # https://www.investopedia.com/ask/answers/122314/what-exponential-moving-average-ema-formula-and-how-ema-calculated.asp\n",
    "        sma = np.mean(values[:, 3])\n",
    "        ema_values = [sma]\n",
    "        k = 2 / (1 + time_period)\n",
    "        for i in range(len(his) - time_period, len(his)):\n",
    "            close = his[i][3]\n",
    "            ema_values.append(close * k + ema_values[-1] * (1 - k))\n",
    "        return ema_values[-1]\n",
    "\n",
    "    technical_indicators = []\n",
    "    for his in ohlcv_histories_normalised:\n",
    "        # note since we are using his[3] we are taking the SMA of the closing price\n",
    "        sma = np.mean(his[:, 3])\n",
    "        macd = calc_ema(his, 12) - calc_ema(his, 26)\n",
    "        technical_indicators.append(np.array([sma]))\n",
    "        # technical_indicators.append(np.array([sma,macd,]))\n",
    "\n",
    "    technical_indicators = np.array(technical_indicators)\n",
    "\n",
    "    tech_ind_scaler = preprocessing.MinMaxScaler()\n",
    "    technical_indicators_normalised = tech_ind_scaler.fit_transform(technical_indicators)\n",
    "\n",
    "    assert ohlcv_histories_normalised.shape[0] == next_day_open_values_normalised.shape[0] == technical_indicators_normalised.shape[0]\n",
    "    return ohlcv_histories_normalised, technical_indicators_normalised, next_day_open_values_normalised, next_day_open_values, y_normaliser\n",
    "\n",
    "\n",
    "def multiple_csv_to_dataset(test_set_name):\n",
    "    import os\n",
    "    ohlcv_histories = 0\n",
    "    technical_indicators = 0\n",
    "    next_day_open_values = 0\n",
    "    for csv_file_path in list(filter(lambda x: x.endswith('daily.csv'), os.listdir('./'))):\n",
    "        if not csv_file_path == test_set_name:\n",
    "            print(csv_file_path)\n",
    "            if type(ohlcv_histories) == int:\n",
    "                ohlcv_histories, technical_indicators, next_day_open_values, _, _ = csv_to_dataset(csv_file_path)\n",
    "            else:\n",
    "                a, b, c, _, _ = csv_to_dataset(csv_file_path)\n",
    "                ohlcv_histories = np.concatenate((ohlcv_histories, a), 0)\n",
    "                technical_indicators = np.concatenate((technical_indicators, b), 0)\n",
    "                next_day_open_values = np.concatenate((next_day_open_values, c), 0)\n",
    "\n",
    "    ohlcv_train = ohlcv_histories\n",
    "    tech_ind_train = technical_indicators\n",
    "    y_train = next_day_open_values\n",
    "\n",
    "    ohlcv_test, tech_ind_test, y_test, unscaled_y_test, y_normaliser = csv_to_dataset(test_set_name)\n",
    "\n",
    "    return ohlcv_train, tech_ind_train, y_train, ohlcv_test, tech_ind_test, y_test, unscaled_y_test, y_normaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
